Як і в LSA, проблеми синонімії та полісемії вирішуються в PLSA. Наприклад, якщо аспект $G_1$ пояснює тему котів, то два документи $\mathbf{X}$ та $\mathbf{Y}$, що містять слова "кіт" та "кошеня" відповідно, матимуть позитивні значення перетвореної координати для аспекту $G_1$. Отже, обчислення подібності між цими документами буде покращено в перетвореному просторі. Слово з кількома значеннями (полісемічне слово) може мати позитивні компоненти в різних аспектах. Наприклад, слово "ягуар" може означати як кота, так і автомобіль. Якщо $G_1$ - це аспект, що пояснює тему котів, а $G_2$ - аспект, що пояснює тему автомобілів, то як $P(\text{"ягуар"}|G_1)$, так і $P(\text{"ягуар"}|G_2)$ можуть бути високими. Однак інші слова в документі забезпечать необхідний контекст для підсилення одного з цих двох аспектів. Документ $\mathbf{X}$, який в основному про котів, матиме високе значення $P(\mathbf{X}|G_1)$, тоді як документ $\mathbf{Y}$, який в основному про автомобілі, матиме високе значення $P(\mathbf{Y}|G_2)$. Це відобразиться в матриці $Q_k = [P(\mathbf{X_i}|G_m)]_{n \times k}$ та новому перетвореному поданні координат $Q_k \Sigma_k$. Отже, обчислення також будуть стійкими щодо врахування ефектів полісемії. Загалом, семантичні концепції будуть підсилені в перетвореному поданні $Q_k \Sigma_k$. Тому багато додатків data mining будуть працювати більш стійко з перетвореним поданням $n \times k$ $Q_k \Sigma_k$, ніж з оригінальною матрицею документ-термін $n \times d$.

## 13.4.2 Використання в кластеризації та порівняння з імовірнісною кластеризацією

Оцінені параметри мають інтуїтивні інтерпретації з точки зору кластеризації. У байєсівській моделі для кластеризації (Рис. 13.3а) генеративний процес оптимізований для кластеризації документів, тоді як генеративний процес у моделюванні тем (Рис. 13.3б) оптимізований для виявлення прихованих семантичних компонентів. Можна показати, що останній кластеризує пари документ-слово, що відрізняється від кластеризації документів. Тому, хоча той самий набір параметрів $P(w_j|G_m)$ та $P(\mathbf{X}|G_m)$ оцінюється в обох випадках, якісно різні результати будуть отримані. Модель на Рис. 13.3а генерує документ з унікального прихованого компонента (кластера), а остаточна м'яка кластеризація є результатом невизначеності в оцінці з спостережуваних даних. З іншого боку, в імовірнісній семантичній моделі різні частини одного й того ж документа можуть бути згенеровані різними аспектами, навіть на рівні генеративного моделювання. Таким чином, документи не генеруються окремими компонентами суміші, а комбінацією компонентів суміші. У цьому сенсі PLSA забезпечує більш реалістичну модель, оскільки різноманітні слова незвичайного документа, що обговорює як котів, так і автомобілі (див. Рис. 13.5), можуть бути згенеровані різними аспектами. У байєсівській кластеризації, хоча такий документ повністю генерується одним із компонентів суміші, він може мати подібні ймовірності призначення (апостеріорні) щодо двох або більше кластерів через невизначеність оцінки. Ця відмінність полягає в тому, що PLSA спочатку була призначена як метод перетворення даних та зменшення розмірності, а не як метод кластеризації. Тим не менш, хороші кластери документів також можуть бути отримані з PLSA. Значення $P(G_m|\mathbf{X_i})$ забезпечує ймовірність призначення документа $\mathbf{X_i}$ до аспекту (або "кластера")