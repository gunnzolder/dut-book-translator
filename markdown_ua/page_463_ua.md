Спостережувана частота появи слова $w_j$ у документі $X_i$ в корпусі. Тоді оцінки на кроці М є такими:

$$
P(X_i|g_m) \propto \sum_{w_j} f(X_i, w_j) \cdot P(g_m|X_i, w_j) \quad \forall i \in \{1 \ldots n\}, m \in \{1 \ldots k\} \tag{13.11}
$$

$$
P(w_j|g_m) \propto \sum_{X_i} f(X_i, w_j) \cdot P(g_m|X_i, w_j) \quad \forall j \in \{1 \ldots d\}, m \in \{1 \ldots k\} \tag{13.12}
$$

$$
P(g_m) \propto \sum_{X_i} \sum_{w_j} f(X_i, w_j) \cdot P(g_m|X_i, w_j) \quad \forall m \in \{1 \ldots k\}. \tag{13.13}
$$

Кожну з цих оцінок можна масштабувати до ймовірності, забезпечивши, щоб вони сумувалися до 1 для всіх результатів цієї випадкової змінної. Це масштабування відповідає константі пропорційності, пов'язаній з позначенням "$\propto$" у вищезгаданих рівняннях. Крім того, ці оцінки можна використовувати для розкладання вихідної матриці документ-термін на добуток трьох матриць, що дуже схоже на SVD/LSA. Цей зв'язок буде досліджено в наступному розділі.

## 13.4.1 Використання для зменшення розмірності та порівняння з латентним семантичним аналізом

Три ключові набори параметрів, оцінених на кроці М, - це $P(X_i|g_m)$, $P(w_j|g_m)$ та $P(g_m)$ відповідно. Ці набори параметрів забезпечують матричне розкладання, подібне до SVD, для $n \times d$ матриці документ-термін $D$. Припустимо, що матриця документ-термін $D$ масштабована константою, щоб сумуватися до сукупної ймовірності 1. Отже, $(i, j)$-й запис $D$ можна розглядати як спостережувану реалізацію ймовірнісної величини $P(X_i, w_j)$. Нехай $Q_k$ - це $n \times k$ матриця, для якої $(i, m)$-й запис є $P(X_i|g_m)$, нехай $\Sigma_k$ - це $k \times k$ діагональна матриця, для якої $m$-й діагональний запис є $P(g_m)$, а $P_k$ - це $d \times k$ матриця, для якої $(j, m)$-й запис є $P(w_j|g_m)$. Тоді $(i, j)$-й запис $P(X_i, w_j)$ матриці $D$ можна виразити через записи вищезгаданих матриць згідно з рівнянням 13.8, яке тут повторюється:

$$
P(X_i, w_j) = \sum_{m=1}^{k} P(g_m) \cdot P(X_i|g_m) \cdot P(w_j|g_m). \tag{13.14}
$$

Ліва частина рівняння дорівнює $(i, j)$-му запису $D$, тоді як права частина рівняння є $(i, j)$-м записом добутку матриць $Q_k \Sigma_k P_k^T$. Залежно від кількості компонентів $k$, права частина може лише наближати матрицю $D$, що позначається як $D_k$. Складаючи $n \times d$ умов рівняння 13.14, отримуємо таке матричне рівняння:

$$
D_k = Q_k \Sigma_k P_k^T. \tag{13.15}
$$

Варто зазначити, що матричне розкладання в рівнянні 13.15 подібне до того, що в SVD/LSA (див. рівняння 2.12 з розділу 2). Отже, як і в LSA, $D_k$ є наближенням матриці документ-термін $D$, а перетворене представлення в $k$-вимірному просторі задається як $Q_k \Sigma_k$. Однак перетворені представлення будуть різними в PLSA та LSA. Це пов'язано з тим, що в двох випадках оптимізуються різні цільові функції. LSA мінімізує середньоквадратичну похибку наближення, тоді як PLSA максимізує логарифмічну правдоподібність відповідності ймовірнісній генеративній моделі. Одна перевага PLSA полягає в тому, що записи $Q_k$ та $P_k$, а також перетворені координатні значення є невід'ємними та мають чітке