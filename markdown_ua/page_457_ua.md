1. Виберіть кластер $G_m$, де $m \in \{1, \ldots, k\}$.

2. Згенеруйте розподіл термінів $G_m$ на основі генеративної моделі. Прикладами таких моделей для тексту є модель Бернуллі або мультиноміальна модель.

Спостережувані дані потім використовуються для оцінки параметрів розподілів Бернуллі або мультиноміальних у генеративному процесі. У цьому розділі буде обговорюватися модель Бернуллі.

Кластеризація виконується ітеративним способом з алгоритмом EM, де призначення кластерів документів визначається з умовних розподілів слів на кроці E з правилом Байєса, а умовні розподіли слів виводяться з призначень кластерів на кроці M. Для ініціалізації документи випадково призначаються до кластерів. Початкові апріорні ймовірності $P(G_m)$ та умовні розподіли ознак $P(w_j|G_m)$ оцінюються зі статистичного розподілу цього випадкового призначення. Класифікатор Байєса використовується для оцінки апостеріорної ймовірності $P(G_m|X)$ на кроці E. Класифікатор Байєса зазвичай використовує або модель Бернуллі, або мультиноміальну модель, яка обговорюється пізніше в цьому розділі. Апостеріорна ймовірність $P(G_m|X)$ класифікатора Байєса може розглядатися як м'яка ймовірність призначення документа $X$ до $m$-ї компоненти суміші $G_m$. Умовний розподіл ознак $P(w_j|G_m)$ для слова $w_j$ оцінюється з цих апостеріорних ймовірностей на кроці M наступним чином:
$$
P(w_j|G_m) = \frac{\sum_X P(G_m|X) \cdot I(X, w_j)}{\sum_X P(G_m|X)} \tag{13.5}
$$

Тут $I(X, w_j)$ є індикаторною змінною, яка приймає значення 1, якщо слово $w_j$ присутнє в $X$, і 0 в іншому випадку. Як і в методі класифікації Байєса, може бути включено той самий підхід згладжування Лапласа для зменшення перенавчання. Апріорні ймовірності $P(G_m)$ для кожного кластера також можуть бути оцінені шляхом обчислення середньої ймовірності призначення всіх документів до $G_m$. Це завершує опис кроку M алгоритму EM. На наступному кроці E ці модифіковані значення $P(w_j|G_m)$ та апріорна ймовірність використовуються для обчислення апостеріорної ймовірності Байєса за стандартним класифікатором Байєса. Отже, наступні два ітераційні кроки повторюються до збіжності:

1. (Крок E) Оцінка апостеріорної ймовірності належності документів до кластерів з використанням правила Байєса:
$$
P(G_m|X) \propto P(G_m) \prod_{w_j \in X} P(w_j|G_m) \prod_{w_j \notin X} (1 - P(w_j|G_m)) \tag{13.6}
$$

Вищезгадане правило Байєса припускає генеративну модель Бернуллі. Зверніть увагу, що рівняння 13.6 ідентичне оцінці апостеріорної ймовірності наївного Байєса для класифікації. Мультиноміальна модель, яка обговорюється пізніше в цьому розділі, також може бути використана. У такому випадку визначення апостеріорної ймовірності рівняння 13.6 замінюється мультиноміальним класифікатором Байєса.

2. (Крок M) Оцінка умовного розподілу $P(w_j|G_m)$ слів (рівняння 13.5) та апріорних ймовірностей $P(G_m)$ різних кластерів з використанням оцінених ймовірностей на кроці E.

В кінці процесу оцінене значення $P(G_m|X)$ надає ймовірність призначення кластера, а оцінене значення $P(w_j|G_m)$ надає розподіл термінів для кожного кластера. Це можна розглядати як ймовірнісний варіант поняття дайджесту кластера, обговореного раніше. Отже, ймовірнісний метод надає подвійне розуміння належності до кластера та слів, релевантних для кожного кластера.