Розглянемо різні алгоритми стилю $k$-медіан для цієї мети. В алгоритмах $k$-медіан вибирається набір $\mathcal{Y}$ з $k$ представників з кожного чанку $S_i$, і кожна точка в $S_i$ призначається до найближчого представника. Метою є вибір представників для мінімізації суми квадратів відстаней (SSQ) призначених даних точок до цих представників. Для набору з $m$ даних точок $X_1 \ldots X_m$ у сегменті $S$ і набору з $k$ представників $\mathcal{Y} = Y_1 \ldots Y_k$, цільова функція визначається наступним чином:
$$
\text{Objective}(S, \mathcal{Y}) = \sum_{X_i \in S, X_i \overset{\leftarrow}{Y_j}} \text{dist}(X_i, Y_j). \tag{12.31}
$$
Оператор призначення позначається $\overset{\leftarrow}{Y_j}$ вище. Квадрат відстані між даною точкою та призначеним їй центром кластера позначається $\text{dist}(X_i, \overline{Y_j})$, де запис даних $X_i$ призначений до представника $\overline{Y_j}$. В принципі, будь-який алгоритм розбиття, такий як $k$-means або $k$-medoids, може бути застосований до сегмента $S_i$ для визначення представників $\overline{Y_1} \ldots \overline{Y_k}$. Для цілей обговорення цей алгоритм буде розглядатися як чорний ящик.

Після обробки першого сегмента $S_1$ ми тепер маємо набір з $k$ медіан, які зберігаються. Кількість точок, призначених до кожного представника, зберігається як "вага" для цього представника. Такі представники вважаються представниками рівня 1. Наступний сегмент $S_2$ обробляється незалежно для знаходження його $k$ оптимальних медіанних представників. Таким чином, після обробки другого сегмента буде $2 \cdot k$ таких представників. Отже, вимога до пам'яті для зберігання представників також зростає з часом, і після обробки $r$ сегментів буде загалом $r \cdot k$ представників. Коли кількість представників перевищує $m$, застосовується друге кластерування до цього набору з $r \cdot k$ точок, за винятком того, що збережені ваги на представниках також використовуються в процесі кластеризації. Результуючі представники зберігаються як представники рівня 2. Загалом, коли кількість представників рівня $p$ досягає $m$, вони перетворюються на $k$ представників рівня $(p + 1)$. Таким чином, процес призведе до збільшення кількості представників усіх рівнів, хоча кількість представників на вищих рівнях буде зростати експоненціально повільніше, ніж на нижчих рівнях. Наприкінці обробки всього потоку даних (або коли виникає конкретна потреба в результаті кластеризації), всі залишкові представники різних рівнів кластеризуються разом в одному фінальному застосуванні підпрограми $k$-медіан.

Конкретний вибір алгоритму, що використовується для задачі $k$-медіан, є критичним для забезпечення високоякісної кластеризації. Іншим фактором, що впливає на якість кінцевого результату, є ефект розкладання задачі на чанки з подальшою ієрархічною кластеризацією. Як таке розкладання задачі впливає на кінцеву якість результату? У роботі STREAM [240] було показано, що кінцева якість результату не може бути довільно гіршою, ніж конкретна підпрограма, яка використовується на проміжному етапі для кластеризації $k$-медіан.

#### Лема 12.4.1
Нехай підпрограма, що використовується для кластеризації $k$-медіан в алгоритмі STREAM, має фактор апроксимації $c$. Тоді алгоритм STREAM матиме фактор апроксимації не гірший за $5 \cdot c$.

Можливі різні рішення для задачі $k$-медіан. В принципі, практично будь-який алгоритм апроксимації може бути використаний як чорний ящик. Особливо ефективним рішенням є задача розміщення об'єктів. Читачеві пропонується звернутися до бібліографічних нотаток для посилань на відповідний підхід.