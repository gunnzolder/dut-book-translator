## 13.5.1 Класифікатори на основі екземплярів

Класифікатори на основі екземплярів працюють на диво добре для тексту, особливо коли виконується попередня фаза кластеризації або зменшення розмірності. Найпростіша форма класифікатора найближчого сусіда повертає домінуючий клас мітки топ-$k$ найближчих сусідів з косинусною подібністю. Зважування голосу з використанням значення косинусної подібності часто забезпечує більш надійні результати. Однак через розріджений і високовимірний характер текстових колекцій, цю базову процедуру можна модифікувати двома способами для підвищення як ефективності, так і ефективності. Перший метод використовує зменшення розмірності у формі латентного семантичного індексування. Другий метод використовує детальну кластеризацію для виконання класифікації на основі центроїдів.

### 13.5.1.1 Використання латентного семантичного аналізу

Основним джерелом помилок у класифікації на основі екземплярів є шум, притаманний текстовим колекціям. Цей шум часто є результатом синонімії та полісемії. Наприклад, слова _comical_ та _hilarious_ означають приблизно одне й те саме. Полісемія означає, що одне й те саме слово може мати два різних значення. Наприклад, слово _jaguar_ може означати автомобіль або кішку. Зазвичай значення слова можна зрозуміти лише в контексті інших слів у документі. Ці характеристики тексту створюють проблеми для алгоритмів класифікації, оскільки обчислення подібності з використанням частот слів може бути не зовсім точним. Наприклад, два документи, що містять слова _comical_ та _hilarious_ відповідно, можуть не вважатися достатньо подібними через ефекти синонімії. У латентному семантичному індексуванні застосовується зменшення розмірності до колекції для зменшення цих ефектів.

Латентний семантичний аналіз (_LSA_) - це підхід, який покладається на сингулярне розкладання (_SVD_) для створення зменшеного представлення текстової колекції. Читачеві рекомендується звернутися до [Розділу 2.4.3.3](#2433) Глави 2 для деталей _SVD_ та _LSA_. Метод латентного семантичного аналізу (_LSA_) є застосуванням методу _SVD_ до $n \times d$ матриці документ-термін $D$, де $d$ - розмір лексикону, а $n$ - кількість документів. Власні вектори з найбільшими власними значеннями квадратної $d \times d$ матриці $D^T D$ використовуються для представлення даних. Розрідженість набору даних призводить до низької внутрішньої розмірності. Тому в текстовій області зменшення розмірності, що виникає внаслідок _LSA_, є досить різким. Наприклад, не рідкість представити корпус, побудований на лексиконі розміром 100 000, менше ніж у 300 вимірах. Видалення вимірів з малими власними значеннями зазвичай призводить до зменшення ефектів шуму синонімії та полісемії. Це представлення даних більше не є розрідженим і нагадує багатовимірні числові дані. На цьому перетвореному корпусі можна використовувати звичайний класифікатор $k$-найближчих сусідів з косинусною подібністю. Однак метод _LSA_ вимагає додаткових зусиль на початку для створення власних векторів.

### 13.5.1.2 Класифікація на основі центроїдів

Класифікація на основі центроїдів є швидкою альтернативою класифікаторам $k$-найближчих сусідів. Основна ідея полягає у використанні готового алгоритму кластеризації для розбиття документів кожного класу на кластери. Кількість кластерів, отриманих з документів кожного класу, пропорційна кількості документів у цьому класі. Це забезпечує, щоб кластери в кожному класі були приблизно однакової зернистості. Мітки класів асоціюються з окремими кластерами, а не з фактичними документами.

Дайджести кластерів з центроїдів витягуються шляхом збереження лише найчастіших слів у цьому центроїді. Зазвичай у кожному центроїді зберігається від 200 до 400 слів. Центроїди