## 13.3.3 Одночасне виявлення кластерів документів та слів

Ймовірнісний алгоритм, обговорений у попередньому розділі, може одночасно виявляти кластери документів та слів. Як обговорювалося в розділі 7.4 глави 7 про методи кластеризації високої розмірності, це важливо у випадку високої розмірності, оскільки кластери найкраще характеризуються одночасно рядками та стовпцями. У текстовій області додатковою перевагою таких методів є те, що тематичні слова кластера надають семантичні знання про цей кластер. Іншим прикладом є метод факторизації невід'ємної матриці, обговорений у розділі 6.8 глави 6. Цей підхід дуже популярний у текстовій області, оскільки факторизовані матриці мають природне тлумачення для текстових даних. Цей підхід може одночасно виявляти кластери слів та кластери документів, які представлені стовпцями двох факторизованих матриць. Це також тісно пов'язано з концепцією ко-кластеризації.

### 13.3.3.1 Ко-кластеризація

Ко-кластеризація найефективніша для невід'ємних матриць, у яких багато записів мають нульові значення. Іншими словами, матриця заповнена рідко. Це випадок для текстових даних. Методи ко-кластеризації також можна узагальнити на щільні матриці, хоча ці методи не є актуальними для текстової області. Ко-кластеризація іноді також називається бі-кластеризацією або кластеризацією двох режимів через її використання обох "режимів" (слів та документів). Хоча метод ко-кластеризації представлений тут у контексті текстових даних, більш широкий підхід також використовується в біологічній області з деякими модифікаціями.

Ідея ко-кластеризації полягає в тому, щоб переупорядкувати рядки та стовпці в матриці даних таким чином, щоб більшість ненульових записів були розташовані в блоках. У контексті текстових даних ця матриця є матрицею документ-термін $n \times d$ $D$, де рядки відповідають документам, а стовпці - словам. Таким чином, $i$-й кластер асоціюється з набором рядків $R_i$ (документи) та набором стовпців $V_i$ (слова). Рядки $R_i$ є непересічними один з одним для різних значень $i$, а стовпці $V_i$ також є непересічними один з одним для різних значень $i$. Таким чином, метод ко-кластеризації одночасно призводить до кластерів документів та кластерів слів. З інтуїтивної точки зору, слова, що представляють стовпці $V_i$, є найбільш релевантними (або тематичними) словами для кластера $R_i$. Набір $V_i$ тому визначає дайджест кластера $R_i$.

У контексті текстових даних кластери слів є такими ж важливими, як і кластери документів, оскільки вони надають уявлення про теми базової колекції. Більшість методів, обговорених у цій книзі для кластеризації документів, таких як метод розсіювання/збирання, ймовірнісні методи та факторизація невід'ємної матриці (див. розділ 6.8 глави 6), виробляють кластери слів (або дайджести кластерів) додатково до кластерів документів. Однак слова в різних кластерах перекриваються в цих алгоритмах, тоді як кластери документів не перекриваються в усіх алгоритмах, крім ймовірнісного (м'якого) методу EM. У ко-кластеризації кластери слів та кластери документів обидва не перекриваються. Кожен документ та слово чітко асоціюються з певним кластером. Одна гарна характеристика ко-кластеризації полягає в тому, що вона явно досліджує дуальність між кластерами слів та кластерами документів. Можна показати, що узгоджені кластери слів індукують узгоджені кластери документів і навпаки. Наприклад, якби змістовні кластери слів були вже доступні, тоді можна було б кластеризувати документи, призначивши кожен документ кластеру слів, з яким він має найбільше спільних слів. У ко-кластеризації метою є зробити це одночасно, щоб кластери слів та кластери документів залежали один від одного оптимальним чином.