У мультиноміальній моделі $L$ термінів у документі розглядаються як вибірки з мультиноміального розподілу. Загальна кількість термінів у документі (або довжина документа) позначається як $L = \sum_{j=1}^{d} a_i$. У цьому випадку вважається, що значення $a_i$ є сирою частотою терміна в документі. Апостеріорні ймовірності класів тестового документа з вектором частот $(a_1 \ldots a_d)$ визначаються та оцінюються за допомогою такого генеративного підходу:

1. Вибрати клас $c$ з класоспецифічною апріорною ймовірністю.

2. Вибрати $L$ термінів з поверненням з розподілу термінів обраного класу $c$. Розподіл термінів визначається за допомогою мультиноміальної моделі. Процес вибірки генерує вектор частот $(a_1 \ldots a_d)$. Вважається, що всі навчальні та тестові документи є спостереженими вибірками цього генеративного процесу. Тому всі параметри моделі генеративного процесу оцінюються з навчальних даних.

3. Класифікація тестового екземпляра: Яка апостеріорна ймовірність того, що клас $c$ був обраний на першому генеративному кроці, за умови спостереженої частоти слів $(a_1 \ldots a_d)$ у тестовому документі?

Коли враховується послідовне впорядкування $L$ різних вибірок, кількість можливих способів вибрати різні терміни, щоб отримати представлення $(a_1 \ldots a_d)$, задається виразом $\frac{L!}{\prod_{i:a_i>0} a_i!}$. Ймовірність кожної з цих послідовностей задається виразом $\prod_{i:a_i>0} p(i, c)^{a_i}$, використовуючи наївне припущення незалежності. У цьому випадку $p(i, c)$ оцінюється як частка появ слова $i$ у класі $c$, включаючи повтори. Отже, на відміну від моделі Бернуллі, повторна присутність слова $i$ у документі, що належить до класу $c$, збільшить $p(i, c)$. Якщо $n(i, c)$ - кількість появ слова $i$ у всіх документах, що належать до класу $c$, тоді $p(i, c) = \frac{n(i, c)}{\sum_i n(i, c)}$. Тоді умовний розподіл ознак класу оцінюється таким чином:

$$
P(x_1 = a_1, \ldots , x_d = a_d | C = c) \approx \frac{L!}{\prod_{i:a_i > 0} a_i !} \prod_{i:a_i > 0} p(i, c)^{a_i}.
\tag{13.20}
$$

Використовуючи правило Байєса, мультиноміальна модель Байєса обчислює апостеріорну ймовірність для тестового документа так:

$$
P(C = c | x_1 = a_1, \ldots , x_d = a_d) \propto P(C = c) \cdot P(x_1 = a_1, \ldots , x_d = a_d | C = c)
\tag{13.21}
$$

$$
\approx P(C = c) \cdot \frac{L!}{\prod_{i:a_i > 0} a_i !} \prod_{i:a_i > 0} p(i, c)^{a_i}
\tag{13.22}
$$

$$
\propto P(C = c) \cdot \prod_{i:a_i > 0} p(i, c)^{a_i}.
\tag{13.23}
$$

Константний множник $\frac{L!}{\prod_{i:a_i > 0} a_i !}$ був вилучений з останньої умови, оскільки він однаковий для всіх класів. Зверніть увагу, що в цьому випадку добуток у правій частині використовує лише ті слова $i$, для яких $a_i$ строго більше за 0. Отже, невиникнення слів ігнорується. У цьому випадку ми припустили, що кожне $a_i$ є сирою частотою слова, яка є цілим числом. Також можливо використовувати мультиноміальну модель Байєса з частотою tf-idf слова, в якій частота $a_i$ може бути дробовою. Однак генеративне пояснення стає менш інтуїтивним у такому випадку.