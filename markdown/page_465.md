
As in LSA, the problems of synonymy and polysemy are addressed by PLSA. For example, if an aspect $G_1$ explains the topic of cats, then two documents $\mathbf{X}$ and $\mathbf{Y}$ containing the words “cat” and “kitten,” respectively, will have positive values of the transformed coordinate for aspect $G_1$. Therefore, similarity computations between these documents will be improved in the transformed space. A word with multiple meanings (polysemous word) may have positive components in different aspects. For example, a word such as “jaguar” can either be a cat or a car. If $G_1$ be an aspect that explains the topic of cats, and $G_2$ is an aspect that explains the topic of cars, then both $P(\text{“jaguar”}|G_1)$ and $P(\text{“jaguar”}|G_2)$ may be highly positive. However, the other words in the document will provide the context necessary to reinforce one of these two aspects. A document $\mathbf{X}$ that is mostly about cats will have a high value of $P(\mathbf{X}|G_1)$, whereas a document $\mathbf{Y}$ that is mostly about cars will have a high value of $P(\mathbf{Y}|G_2)$. This will be reflected in the matrix $Q_k = [P(\mathbf{X_i}|G_m)]_{n \times k}$ and the new transformed coordinate representation $Q_k \Sigma_k$. Therefore, the computations will also be robust in terms of adjusting for polysemy effects. In general, semantic concepts will be amplified in the transformed representation $Q_k \Sigma_k$. Therefore, many data mining applications will perform more robustly in terms of the $n \times k$ transformed representation $Q_k \Sigma_k$ rather than the original $n \times d$ document-term matrix.

## 13.4.2 Use in Clustering and Comparison with Probabilistic Clustering

The estimated parameters have intuitive interpretations in terms of clustering. In the Bayes model for clustering (Fig. 13.3a), the generative process is optimized to clustering documents, whereas the generative process in topic modeling (Fig. 13.3b) is optimized to discovering the latent semantic components. The latter can be shown to cluster document–word pairs, which is different from clustering documents. Therefore, although the same parameter set $P(w_j|G_m)$ and $P(\mathbf{X}|G_m)$ is estimated in the two cases, qualitatively different results will be obtained. The model of Fig. 13.3a generates a document from a unique hidden component (cluster), and the final soft clustering is a result of uncertainty in estimation from observed data. On the other hand, in the probabilistic latent semantic model, different parts of the same document may be generated by different aspects, even at the generative modeling level. Thus, documents are not generated by individual mixture components, but by a combination of mixture components. In this sense, PLSA provides a more realistic model because the diverse words of an unusual document discussing both cats and cars (see Fig. 13.5) can be generated by distinct aspects. In Bayes clustering, even though such a document is generated in entirety by one of the mixture components, it may have similar assignment (posterior) probabilities with respect to two or more clusters because of estimation uncertainty. This difference is because PLSA was originally intended as a data transformation and dimensionality reduction method, rather than as a clustering method. Nevertheless, good document clusters can usually be derived from PLSA as well. The value $P(G_m|\mathbf{X_i})$ provides an assignment probability of the document $\mathbf{X_i}$ to aspect (or “cluster”)
