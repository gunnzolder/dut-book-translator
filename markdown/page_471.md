
## 13.5.3 SVM Classifiers for High-Dimensional and Sparse Data

The number of terms in the Lagrangian dual of the SVM formulation scales with the square of the number of dimensions. The reader is advised to refer to Sect. 10.6 of Chap. 10, and 11.4.2 of Chap. 11 for the relevant discussions. While the SVMLight method in Sect. 11.4.2 of Chap. 11 addresses this issue by making changes to the algorithmic approach, it does not make modifications to the SVM formulation itself. Most importantly, the approach does not make any modifications to address the high dimensional and sparse nature of text data.

The text domain is high dimensional and sparse. Only a small subset of the dimensions take on nonzero values for a given text document. Furthermore, linear classifiers tend to work rather well for the text domain, and it is often not necessary to use the kernelized version of the classifier. Therefore, it is natural to focus on linear classifiers, and ask whether it is possible to improve the complexity of SVM classification further by using the special domain-specific characteristics of text. SVMPerf is a linear-time algorithm designed for text classification. Its training complexity is $O(n \cdot s)$, where $s$ is the average number of nonzero attributes per training document in the collection.

To explain the approach, we first briefly recap the soft penalty-based SVM formulation introduced in Sect. 10.6 of Chap. 10. The problem definition, referred to as the optimization formulation (OP1), is as follows:

(OP1): Minimize 
$$
\frac{\|W\|^2}{2} + C \frac{\sum_{i=1}^n \xi_i}{n}
$$
subject to:
$$
y_i W \cdot X_i \geq 1 - \xi_i \quad \forall i
$$
$$
\xi_i \geq 0 \quad \forall i.
$$

One difference from the conventional SVM formulation of Chap. 10 is that the constant term $b$ is missing. The conventional SVM formulation uses the constraint $y_i (W \cdot X_i + b) \geq 1 - \xi_i$. The two formulations are, however, equivalent because it can be shown that adding a dummy feature with a constant value of 1 to each training instance has the same effect. The coefficient in $W$ of this feature will be equal to $b$. Another minor difference from the conventional formulation is that the slack component in the objective function is scaled by a factor of $n$. This is not a significant difference either because the constant $C$ can be adjusted accordingly. These minor variations in the notation are performed without loss of generality for algebraic simplicity.

The SVMPerf method reformulates this problem with a single slack variable $\xi$, and $2^n$ constraints that are generated by summing a random subset of the $n$ constraints in (OP1). Let $U = (u_1 \ldots u_n) \in \{0, 1\}^n$ represent the indicator vector for the constraints that are summed up to create this new synthetic constraint. An alternative formulation of the SVM model is as follows:

(OP2): Minimize 
$$
\frac{\|W\|^2}{2} + C \xi
$$
subject to:
$$
\frac{1}{n} \sum_{i=1}^n u_i y_i W \cdot X_i \geq \frac{\sum_{i=1}^n u_i}{n} - \xi \quad \forall U \in \{0, 1\}^n
$$
$$
\xi \geq 0.
$$
