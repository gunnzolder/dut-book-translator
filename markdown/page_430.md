
The intuition behind this result is quite simple. For a uniformly distributed hash function, the probability of \( R \) trailing zeros in the binary representation of a stream element is equal to \( 2^{-R-1} \). Therefore, for \( n \) distinct elements and a fixed value of \( R \), the expected number of times that exactly \( R \) trailing zeros are achieved is equal to \( 2^{-R-1} \cdot n \). Therefore, for values of \( R \) larger than \( \log(n) \), the expected number of such bitstrings falls off exponentially to less than 1. Of course, in our application, the value of \( R \) is not fixed, but it is a random variable that is generated by the outcome of the hash function. It has been rigorously shown that the expected value \( E[R_{\text{max}}] \) of the maximum value of \( R \) over all stream elements is logarithmically related to the number of distinct elements as follows:

$$
E[R_{\text{max}}] = \log_2 (\phi n), \quad \phi = 0.77351.
$$

(12.30)

The standard deviation is \( \sigma (R_{\text{max}}) = 1.12 \). Therefore, the value of \( 2^{R_{\text{max}}} / \phi \) provides an estimate for the number of distinct elements \( n \). To further improve the estimate of \( R_{\text{max}} \), the following techniques can be used:

1. Multiple hash functions can be used, and the average value of \( R_{\text{max}} \) over the different hash functions is used.

2. The averages are still somewhat susceptible to large variations. Therefore, the “mean-median trick” may be used. The medians of a set of averages are reported. Note that this is similar to the trick used in the AMS sketch. As in that case, a combination of the Chebychev inequality and Chernoff bounds can be used to establish qualitative guarantees.

It should be pointed out that the bloom filter can also be used to estimate the number of distinct elements. However, the bloom filter is not a space-efficient way to count the number of distinct elements when set-membership queries are not required.

## 12.3 Frequent Pattern Mining in Data Streams

The frequent pattern mining problem in data streams is studied in the context of two different scenarios. The first scenario is the massive-domain scenario, in which the number of possible items is very large. In such cases, even the problem of finding frequent items becomes difficult. Frequent items are also referred to as heavy hitters. The second case is the conventional scenario of a large (but manageable) number of items that fit in main memory. In such cases, the frequent item problem is no longer quite as interesting, because the frequent counts can be directly maintained in an array. In such cases, one is more interested in determining frequent patterns. This is a difficult problem, because most frequent pattern mining algorithms require multiple passes over the entire data set. The one-pass constraint of the streaming scenario makes this difficult. In the following, two different approaches will be described. The first of these approaches leverages generic synopsis structures in conjunction with traditional frequent pattern mining algorithms and the second designs streaming versions of frequent pattern mining algorithms.

### 12.3.1 Leveraging Synopsis Structures

Synopsis structures can be used effectively in most streaming data mining problems, including frequent pattern mining. In the context of frequent pattern mining methods, synopsis structures are particularly attractive because of the ability to use a wider array of algorithms, or for incorporating temporal decay into the frequent pattern mining process.
